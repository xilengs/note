## 权重衰减

防止模型过拟合的技术。
在训练的每一步用梯度更新参数时，同时缩小参数值。防止参数的绝对值过大。

具体做法：
$w_t = w_t - lr \times g_w - lr \times \lambda w_t$
其中$lr$是学习率，$\lambda$是权重衰减系数，一般取值在1e-2到1e-4。如果过拟合现象比较严重，$\lambda$就设置大一些。

### $PyTorch$里的权重衰减

$PyTorch$里一般在定义优化器时，可以同步设置$weight \ decay$。并指定$\lambda$的值。
```python
optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)
```

### 权重衰减和L2正则的关系

对标准的梯度下降，权重衰减和L2正则的效果是一样的：
**L2正则：**
$L = L_{error} \ + \  \frac{\lambda}{2}w^2$
更新参数w，等于w减去学习率$lr$乘以L对w的偏导数：
$w = w \ - \ lr\frac{\partial L}{\partial w}$
$\ = \ w \ - \ lr\frac{\partial L_{error}}{\partial w} \ - \ lr \times \lambda w$
**权重衰减：**
$L = L_{error}$
更新参数w：
$w \ = \ w \ - \ lr\frac{\partial L_{error}}{\partial w} \ - \ lr \times \lambda w$

但对于Adam优化器，权重衰减和L2正则就不一样了。

