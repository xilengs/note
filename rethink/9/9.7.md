## Dropout

Dropout也是一种解决**模型过拟合问题**的方法，它是从简化网络结构的方面进行正则化的。

按照超参数p(禁用概率)的设置，随机禁用神经元。被禁用的神经元不接受输入，也不提供输出。

通过这种方法，训练一个简化的神经网络，让网络每个神经元都学到通用的特征，**防止神经网络过分依赖某几个神经元**。

### $PyTorch$里的$Dropout$

```python
# 模型定义
# 理论上每层的Dropout都可以设置不同的p，甚至不设置Dropout
class NeuralNetwork(nn.module):
	def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
        	nn.Linear(28 * 28, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 10)
        )
	def forward(self, x):
        return self.model(x)
```

p的值一般取0.1到0.5，p越大，正则化效果越强。
Dropout在训练和推理阶段的行为是不同的，推理阶段Dropout不使用，所以在训练前调用$model.train()$，推理前调用$model.eval()$。

